{
  "embedding_dim": "The embedding dimension determines the size of the vector representation of words. It is a crucial parameter for text-based models.",
  "transformer_dim": "The transformer dimension refers to the dimensionality of the transformer layer in the model. It affects the model's capacity to learn complex patterns.",
  "transformer_dropout": "The transformer dropout is a regularization technique that randomly sets a fraction of the input units to 0 at each update during training. It helps prevent overfitting.",
  "lstm_units_1": "LSTM units 1 defines the number of memory cells in the first LSTM layer of the model. It impacts the model's ability to capture temporal dependencies in the data.",
  "dropout_1": "Dropout 1 is a regularization technique applied to the output of the first LSTM layer. It helps prevent overfitting by randomly setting a fraction of the input units to 0.",
  "lstm_units_2": "LSTM units 2 determines the number of memory cells in the second LSTM layer of the model. It allows the model to capture additional temporal dependencies.",
  "dense_units": "Dense units refers to the number of neurons in the dense layer of the model. It affects the model's capacity to learn complex patterns and make predictions.",
  "dropout_2": "Dropout 2 is a regularization technique applied to the output of the dense layer. It helps prevent overfitting by randomly setting a fraction of the input units to 0.",
  "optimizer": "The optimizer defines the algorithm used to update the model's parameters during training. Common optimizers include Adam, RMSprop, and SGD.",
  "loss": "The loss function measures the discrepancy between the predicted output and the actual output. It guides the model to minimize the error during training.",
  "epochs": "Epochs represent the number of times the model will iterate over the entire training dataset during training.",
  "batch_size": "The batch size defines the number of samples processed before the model's parameters are updated. It affects the training speed and memory usage.",
  "l1": "L1 regularization is a technique that adds a penalty to the loss function based on the absolute values of the model's parameters. It encourages sparsity in the model.",
  "l2": "L2 regularization is a technique that adds a penalty to the loss function based on the squared values of the model's parameters. It helps prevent overfitting.",
  "output_activation": "The output activation function determines the format of the model's predictions. Common activation functions include sigmoid, softmax, and linear.",
  "lr": "The learning rate controls the step size at each iteration during the optimization process. It affects the speed and stability of the model's convergence."
}